
## Boosting (Ensamble secuencial)

Es un algoritmo de aprendizaje supervisado que construye de forma iterativa un modelo fuerte mediante la combinaci칩n de m칰ltiples estimadores d칠biles (generalmente 치rboles de decisi칩n de baja profundidad). Su objetivo es minimizar una funci칩n de p칠rdida global, donde cada nuevo modelo se entrena sobre los residuos (errores) del modelo acumulado hasta el momento.


o en palabras mas sencillas:

> Es una t칠cnica de aprendizaje en equipo 游뱋 donde muchos modelos peque침os y simples (llamados "치rboles de decisi칩n" 游꺕) trabajan juntos para resolver un problema dif칤cil. Lo especial es que no trabajan todos a la vez, sino en orden: cada nuevo 치rbol que creamos tiene la misi칩n espec칤fica de corregir los errores que cometi칩 el 치rbol anterior.

este se caracteriza por solo siguiente cosas:

- **Refinamiento sucesivo**: El modelo aprende de sus equivocaciones paso a paso 游늳.
> En lugar de entrenar todos los 치rboles al mismo tiempo, el Boosting los crea uno por uno. Cuando un 치rbol se equivoca al predecir un dato, el siguiente 치rbol le presta mucha m치s atenci칩n a ese error espec칤fico para intentar corregirlo. Es como un estudiante que repasa solo las preguntas que fall칩 en el examen anterior hasta que deja de cometer errores.


- **Control de velocidad (Learning Rate)**: No intentamos corregir todo el error de golpe, sino que avanzamos con pasos peque침os para no nos pasarnos de largo 游냑.
> Es un factor (llamado "eta") que reduce la influencia de cada nuevo 치rbol. Si un 치rbol encuentra una soluci칩n, no la aplicamos al 100%, sino quiz치s solo un 10%. Esto hace que el aprendizaje sea m치s lento pero m치s seguro, evitando que el modelo tome decisiones dr치sticas basadas en datos que podr칤an ser ruido o errores aislados.

Matem치ticamente, el **Learning Rate** act칰a como un "amortiguador". Obliga al algoritmo a necesitar muchos 치rboles peque침os para llegar a la soluci칩n, asegurando que el camino hacia la respuesta sea suave y no dependa de los caprichos de un solo 치rbol.

La f칩rmula se ve as칤:

$$F_{m}(x) = F_{m-1}(x) + \eta \cdot h_{m}(x)$$

### 쯈u칠 significa cada parte?

*   **$F_{m}(x)$ (El nuevo resultado):** Es la predicci칩n mejorada despu칠s de a침adir el nuevo 치rbol.
*   **$F_{m-1}(x)$ (Lo que ya sab칤amos):** Es la predicci칩n acumulada de todos los 치rboles anteriores.
*   **$h_{m}(x)$ (La nueva sugerencia):** Es lo que el nuevo 치rbol "cree" que debe sumar para corregir el error.
*   **$\eta$ (El factor "eta" o Learning Rate):** Es un n칰mero peque침o (ej. 0.1) que decide cu치nto caso le hacemos al nuevo 치rbol.

---

### La l칩gica detr치s de la f칩rmula:

1.  **Sin el factor ($\eta = 1$):** Si no us치ramos "eta", la f칩rmula ser칤a simplemente $F_{m} = F_{m-1} + h_{m}$. El modelo aceptar칤a el 100% de lo que dice el nuevo 치rbol. Esto es peligroso porque si el 치rbol se equivoca por culpa de un dato extra침o (ruido), el modelo entero se desviar치 mucho.
    
2.  **Con el factor ($\eta = 0.1$):** Al multiplicar la sugerencia del 치rbol por $0.1$, le decimos al modelo: *"Conf칤o en tu direcci칩n, pero solo vamos a avanzar un 10% de lo que sugieres"*. 

**En resumen:** Matem치ticamente, el **Learning Rate** act칰a como un "amortiguador". Obliga al algoritmo a necesitar muchos 치rboles peque침os para llegar a la soluci칩n, asegurando que el camino hacia la respuesta sea suave y no dependa de los caprichos de un solo 치rbol.

- **Simplicidad (Regularizaci칩n)**: Obligamos a los 치rboles a ser sencillos para que aprendan patrones generales y no se memoricen los datos de memoria (evitando el sobreajuste) 游.
> Para que el Boosting funcione bien, los 치rboles individuales deben ser "d칠biles" (cortos y con pocas ramas). Si un 치rbol es demasiado complejo, se memoriza los datos exactos (sobreajuste). Al limitarlos, obligamos al sistema a que la inteligencia venga de la suma de muchos 치rboles sencillos y no de uno solo que se crea sabelotodo.


### 쮼n d칩nde se utiliza el Boosting en la vida real? 
Gracias a su precisi칩n y capacidad para manejar datos complejos, el Boosting es el "rey" en industrias que manejan mucha informaci칩n organizada en tablas. Aqu칤 algunos ejemplos:

1. **Detecci칩n de Fraude Bancario**
Los bancos lo usan para decidir en milisegundos si una transacci칩n con tarjeta es leg칤tima o un robo.
*   **쮺칩mo ayuda el Boosting?** Al aprender de errores pasados, el modelo se vuelve experto en notar patrones muy sutiles que un humano (o un modelo simple) ignorar칤a, como una compra peque침a en un pa칤s inusual seguida de una compra grande.

2. **Sistemas de Recomendaci칩n (Netflix, Amazon, YouTube)**
Cuando una plataforma te dice "Porque viste esta pel칤cula, te recomendamos esta otra", suele haber un algoritmo de Boosting detr치s.
*   **쮺칩mo ayuda el Boosting?** Analiza cientos de variables (qu칠 g칠neros te gustan, cu치nto tiempo viste un video, a qu칠 hora te conectas) para predecir con mucha exactitud qu칠 producto o video tiene m치s probabilidad de que le des clic.


3. **Evaluaci칩n de Riesgo Crediticio**
Cuando pides un pr칠stamo, el algoritmo decide si eres apto o no.
*   **쮺칩mo ayuda el Boosting?** Es extremadamente preciso para evaluar el riesgo. Combina factores como tu historial de pagos, tu nivel de ingresos y tu edad para dar un puntaje de confianza. Es el m칠todo est치ndar en las *Fintech* modernas.


## Video de Presentaci칩n
https://github.com/user-attachments/assets/8882a5ae-0093-4b0b-907f-ddb2f78e5322


